import json
import os
import time
from data_fetcher import fetch_recent_papers
from data_analyzer import analyze_one_paper

DB_FILE = "processed_probes.json"

def main():
    print("ğŸš€ Starting Daily Update...")

    # 1. è¯»å–æ—§æ•°æ®
    if os.path.exists(DB_FILE):
        with open(DB_FILE, "r", encoding="utf-8") as f:
            old_data = json.load(f)
    else:
        old_data = []
    
    existing_dois = set(item['doi'] for item in old_data)
    print(f"ğŸ“š Existing database has {len(old_data)} entries.")

    # 2. çˆ¬å–æ–°æ•°æ® (è¿‡å» 5 å¤©ï¼Œç¡®ä¿ä¸æ¼)
    new_papers = fetch_recent_papers(days_back=5)
    print(f"ğŸŒ Fetched {len(new_papers)} recent papers from PubMed.")

    # 3. ç­›é€‰æœªå¤„ç†çš„
    to_process = [p for p in new_papers if p['doi'] not in existing_dois]
    print(f"ğŸ” Found {len(to_process)} NEW papers to analyze.")

    if not to_process:
        print("ğŸ’¤ No new papers to process. Exiting.")
        return

    # 4. AI åˆ†æ
    added_count = 0
    for paper in to_process:
        print(f"ğŸ¤– Analyzing: {paper['title'][:30]}...")
        result = analyze_one_paper(paper)
        
        if result and result.get('is_new'):
            print(f"   âœ… NEW PROBE: {result['probe_name']}")
            merged_entry = {**paper, **result}
            old_data.append(merged_entry) # åŠ å…¥æ€»è¡¨
            added_count += 1
        else:
            print("   - Pass")
        
        time.sleep(0.5) # ç¨å¾®æ…¢ç‚¹

    # 5. ä¿å­˜æ›´æ–°
    if added_count > 0:
        with open(DB_FILE, "w", encoding="utf-8") as f:
            json.dump(old_data, f, indent=4, ensure_ascii=False)
        print(f"ğŸ‰ Database updated! Added {added_count} new probes.")
    else:
        print("ğŸ’¨ Analysis complete, but no new probes found.")

if __name__ == "__main__":
    main()