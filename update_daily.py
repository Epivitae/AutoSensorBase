import json
import os
import time
from data_fetcher import fetch_broad_probe_papers
from data_analyzer import analyze_one_paper

RAW_FILE = "raw_papers.json"
PROCESSED_FILE = "processed_probes.json"

def load_json(filename):
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_json(filename, data):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

def normalize_doi(doi_string):
    """ç»Ÿä¸€æ¸…æ´— DOI çš„è¾…åŠ©å‡½æ•°"""
    if not doi_string: return ""
    return doi_string.lower().strip().replace("https://doi.org/", "").replace("http://doi.org/", "")

def main():
    print("ğŸš€ [ETL] Starting Daily Pipeline...")

    # ================= 1. æŠ“å–é˜¶æ®µ =================
    raw_data = load_json(RAW_FILE)
    
    # å»ºç«‹ Raw ç´¢å¼•
    existing_raw_dois = set()
    for item in raw_data:
        existing_raw_dois.add(normalize_doi(item.get('doi')))
    
    print("ğŸŒ Fetching from PubMed (5 days)...")
    candidates = fetch_broad_probe_papers(days_back=5)
    
    new_raw_count = 0
    for p in candidates:
        clean_doi = normalize_doi(p['doi'])
        if clean_doi not in existing_raw_dois:
            p['ai_analyzed'] = False 
            p['is_probe'] = False
            raw_data.append(p)
            existing_raw_dois.add(clean_doi)
            new_raw_count += 1
            
    if new_raw_count > 0:
        print(f"ğŸ“¥ Staged {new_raw_count} new papers to {RAW_FILE}")
        save_json(RAW_FILE, raw_data)
    else:
        print("ğŸ’¤ No new raw papers found.")

    # ================= 2. åˆ†æé˜¶æ®µ =================
    pending = [p for p in raw_data if not p.get('ai_analyzed')]
    print(f"â³ Pending Analysis Queue: {len(pending)} papers")
    
    if not pending:
        print("âœ… All caught up. Workflow finished.")
        return

    processed_data = load_json(PROCESSED_FILE)
    
    # å»ºç«‹ Processed ç´¢å¼• (ç”¨äºå¿«é€ŸæŸ¥æ‰¾æ˜¯å¦å­˜åœ¨)
    processed_dois_set = set()
    for item in processed_data:
        processed_dois_set.add(normalize_doi(item.get('doi')))
    
    # æ‰¹å¤„ç†å¤§å°
    BATCH_SIZE = 800 
    batch = pending[:BATCH_SIZE]
    
    analyzed_count = 0
    new_probe_count = 0
    updated_probe_count = 0

    for paper in batch:
        print(f"ğŸ¤– Analyzing: {paper['title'][:50]}...")
        try:
            result = analyze_one_paper(paper)
            
            # æ— è®ºç»“æœå¦‚ä½•ï¼ŒRawçŠ¶æ€éƒ½æ›´æ–°ä¸ºå·²åˆ†æ
            paper['ai_analyzed'] = True
            
            if result and result.get('is_new'):
                # å‡†å¤‡æœ€ç»ˆæ•°æ®å¯¹è±¡
                final_entry = {**paper, **result}
                final_entry.pop('ai_analyzed', None) # æ¸…ç†æ‰å†…éƒ¨æ ‡è®°
                final_entry.pop('is_probe', None)
                
                # è·å–å½“å‰ DOI
                current_doi = normalize_doi(paper.get('doi'))
                
                # === æ ¸å¿ƒä¿®æ”¹ï¼šè¦†ç›–é€»è¾‘ ===
                if current_doi in processed_dois_set:
                    print(f"   ğŸ”„ UPDATE EXISTING: {result.get('probe_name')} (Overwriting old data)")
                    
                    # éå†åˆ—è¡¨æ‰¾åˆ°é‚£ä¸ªæ—§çš„ä½ç½®ï¼Œç„¶åæ›¿æ¢å®ƒ
                    for idx, item in enumerate(processed_data):
                        if normalize_doi(item.get('doi')) == current_doi:
                            processed_data[idx] = final_entry # <--- è¦†ç›–ï¼
                            break
                    updated_probe_count += 1
                else:
                    print(f"   ğŸ‰ NEW PROBE: {result.get('probe_name')}")
                    processed_data.append(final_entry) # <--- æ–°å¢
                    processed_dois_set.add(current_doi)
                    new_probe_count += 1
                
                paper['is_probe'] = True
            else:
                print("   âŒ Rejected")
                paper['is_probe'] = False
            
            analyzed_count += 1
            time.sleep(1) 
            
        except Exception as e:
            print(f"   âš ï¸ Analysis Error: {e}")
            continue

    # ================= 3. ä¿å­˜é˜¶æ®µ =================
    if analyzed_count > 0:
        save_json(RAW_FILE, raw_data)
        save_json(PROCESSED_FILE, processed_data)
        print(f"ğŸ’¾ Saved updates. (+{new_probe_count} new, ğŸ”„ {updated_probe_count} updated)")

if __name__ == "__main__":
    main()
