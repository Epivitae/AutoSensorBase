import json
import os
import time
from Bio import Entrez
from data_analyzer import analyze_one_paper

# é…ç½®
Entrez.email = "wangk@ion.ac.cn"
DB_FILE = "processed_probes.json"
TARGET_YEAR_START = "2023/01/01"
TARGET_YEAR_END = "2023/12/31"

def fetch_2023_candidates():
    """ä¸“é—¨æŠ“å– 2023 å¹´å…¨å¹´çš„æ•°æ®"""
    print(f"ğŸŒ æ­£åœ¨æœç´¢ PubMed ({TARGET_YEAR_START} - {TARGET_YEAR_END})...")
    
    # === å¤ç”¨ä½ æœ€æ–°çš„æ£€ç´¢ç­–ç•¥ ===
    core_tech = '("Genetically encoded"[Title/Abstract] OR "Fluorescent protein"[Title/Abstract] OR "Fluorescent protein-based"[Title/Abstract] OR "Bioluminescent"[Title/Abstract] OR "Chemogenetic"[Title/Abstract])'
    function_terms = '("Sensor"[Title/Abstract] OR "Indicator"[Title/Abstract] OR "Probe"[Title/Abstract] OR "Reporter"[Title/Abstract] OR "Biosensor"[Title/Abstract])'
    specific_families = '("GCaMP" OR "GECI" OR "GEVI" OR "iSnFR" OR "GRAB" OR "dLight" OR "FRET biosensor" OR "BRET biosensor" OR "cpGFP" OR "CaBLAM")'
    noise_filter = 'NOT ("Wastewater" OR "Pollutant" OR "Review"[Publication Type])'

    # è¿™é‡Œçš„å…³é”®æ˜¯åŠ å…¥æ—¥æœŸèŒƒå›´é™å®š
    date_query = f'("{TARGET_YEAR_START}"[Date - Publication] : "{TARGET_YEAR_END}"[Date - Publication])'
    
    search_query = f'(({core_tech} AND {function_terms}) OR {specific_families}) {noise_filter} AND {date_query}'
    
    try:
        # æœç´¢ ID (è¿™é‡Œ retmax è®¾å¤§ä¸€ç‚¹ï¼Œé˜²æ­¢æ¼æ‰)
        handle = Entrez.esearch(db="pubmed", term=search_query, retmax=2000)
        record = Entrez.read(handle)
        handle.close()
        id_list = record["IdList"]
        print(f"ğŸ“¦ PubMed è¿”å›äº† {len(id_list)} ç¯‡ 2023 å¹´çš„å€™é€‰æ–‡çŒ® IDã€‚")
        
        if not id_list: return []

        # æ‰¹é‡è·å–è¯¦æƒ…
        print("ğŸ“¥ æ­£åœ¨ä¸‹è½½æ–‡çŒ®å…ƒæ•°æ®...")
        handle = Entrez.efetch(db="pubmed", id=id_list, rettype="medline", retmode="xml")
        papers_data = Entrez.read(handle)
        handle.close()
        
        results = []
        dev_keywords = ["develop", "engineer", "design", "novel", "new", "variant", "optimize"]

        for paper in papers_data['PubmedArticle']:
            article = paper['MedlineCitation']['Article']
            title = article.get('ArticleTitle', 'No Title')
            
            doi = "No DOI"
            for eid in article.get('ELocationID', []):
                if eid.attributes.get('EIdType') == 'doi':
                    doi = str(eid)
            
            # å¿…é¡»è¦æœ‰ DOI æ‰æœ‰ä»·å€¼
            if doi == "No DOI": continue

            abstract_list = article.get('Abstract', {}).get('AbstractText', [])
            abstract = " ".join(abstract_list) if isinstance(abstract_list, list) else str(abstract_list)
            if not abstract: abstract = "[No Abstract]"

            # ç®€å•çš„æœ¬åœ°å…³é”®è¯åˆç­› (åªä¿ç•™çœ‹èµ·æ¥åƒå¼€å‘çš„)
            text = (title + " " + abstract).lower()
            # if any(kw in text for kw in dev_keywords): # å¦‚æœæƒ³è·‘å…¨é‡ï¼Œå¯ä»¥æ³¨é‡Šæ‰è¿™è¡Œç­›é€‰
            results.append({
                "title": title,
                "abstract": abstract,
                "doi": f"https://doi.org/{doi}",
                "journal": article.get('Journal', {}).get('Title', 'Unknown'),
                "date": "2023"
            })
            
        return results

    except Exception as e:
        print(f"âŒ æœç´¢å‡ºé”™: {e}")
        return []

def main():
    # 1. è¯»å–ç°æœ‰æ•°æ®åº“
    if os.path.exists(DB_FILE):
        with open(DB_FILE, "r", encoding="utf-8") as f:
            current_data = json.load(f)
    else:
        current_data = []
    
    # å»ºç«‹å·²å­˜åœ¨ DOI çš„é›†åˆ (ç”¨äºå»é‡)
    existing_dois = set()
    for item in current_data:
        d = item.get('doi') or item.get('DOI')
        if d: existing_dois.add(d.lower().strip().replace("https://doi.org/", ""))

    print(f"ğŸ“š æœ¬åœ°å·²æœ‰æ•°æ®: {len(current_data)} æ¡")

    # 2. æŠ“å– 2023 æ•°æ®
    candidates = fetch_2023_candidates()
    
    # 3. ç­›é€‰å‡ºçœŸæ­£éœ€è¦ AI è·‘çš„æ–°æ•°æ®
    to_process = []
    for p in candidates:
        clean_doi = p['doi'].lower().replace("https://doi.org/", "").strip()
        if clean_doi not in existing_dois:
            to_process.append(p)
            
    print(f"\nğŸ” ç»è¿‡å»é‡ï¼Œå‘ç° {len(to_process)} ç¯‡ 2023 å¹´çš„æ–°æ–‡çŒ®éœ€è¦ AI åˆ†æã€‚")
    
    if not to_process:
        print("âœ… æ‰€æœ‰æ–‡çŒ®éƒ½å·²å­˜åœ¨ï¼Œæ— éœ€æ›´æ–°ã€‚")
        return

    # âš ï¸ ç¡®è®¤æç¤º (é˜²æ­¢ä¸€æ¬¡æ€§æ‰£å¤ªå¤šé’±)
    print("âš ï¸  æ³¨æ„ï¼šè¿™å°†è°ƒç”¨ AI æ¥å£è¿›è¡Œåˆ†æã€‚")
    confirm = input(f"   è¾“å…¥ 'y' å¼€å§‹åˆ†æè¿™ {len(to_process)} ç¯‡æ–‡çŒ®ï¼Œè¾“å…¥ 'n' é€€å‡º: ")
    if confirm.lower() != 'y':
        return

    # 4. å¼€å§‹å¤„ç† (å¸¦è¿›åº¦ä¿å­˜)
    print("\nğŸš€ å¼€å§‹ AI åˆ†ææµæ°´çº¿...")
    new_probes_count = 0
    
    for i, paper in enumerate(to_process):
        print(f"   [{i+1}/{len(to_process)}] {paper['title'][:60]}...")
        
        try:
            # AI åˆ†æ
            result = analyze_one_paper(paper)
            
            if result and result.get('is_new'):
                print(f"      ğŸ‰ å‘ç°æ–°æ¢é’ˆ: {result.get('probe_name')}")
                # åˆå¹¶æ•°æ®
                merged = {**paper, **result}
                current_data.append(merged)
                new_probes_count += 1
            else:
                print("      Pass (éæ–°æ¢é’ˆå¼€å‘)")
            
            # é¢‘ç‡é™åˆ¶
            time.sleep(1)

            # === æ–­ç‚¹ç»­ä¼ æœºåˆ¶ï¼šæ¯ 5 ç¯‡ä¿å­˜ä¸€æ¬¡ ===
            if (i + 1) % 5 == 0:
                print("      ğŸ’¾ (è‡ªåŠ¨ä¿å­˜è¿›åº¦...)")
                with open(DB_FILE, "w", encoding="utf-8") as f:
                    json.dump(current_data, f, indent=4, ensure_ascii=False)
                    
        except Exception as e:
            print(f"      âŒ å¤„ç†å‡ºé”™: {e}")
            continue

    # 5. æœ€åå†ä¸€æ¬¡ä¿å­˜
    with open(DB_FILE, "w", encoding="utf-8") as f:
        json.dump(current_data, f, indent=4, ensure_ascii=False)
    
    print(f"\nğŸ‰ 2023æ•°æ®å›å¡«å®Œæˆï¼å…±æ–°å¢ {new_probes_count} ä¸ªæ¢é’ˆã€‚")

if __name__ == "__main__":
    main()